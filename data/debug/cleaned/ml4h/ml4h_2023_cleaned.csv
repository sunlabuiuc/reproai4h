year,title,cleaned_title,authors,processed_emails,abstract,code_count,gitlab_count,zenodo_count,dataset_count,mimic_count,eicu_count,uk_biobank_count,chest_x-ray14_count,adni_count,physionet_count,oasis_count,tcga_count,gdc_count,seer_count,tuh_eeg_corpus_count,tuh_abnormal_eeg_corpus_count,tuh_eeg_artifact_corpus_count,tuh_eeg_epilepsy_corpus_count,tuh_eeg_events_corpus_count,tuh_eeg_seizure_corpus_count,tuh_eeg_slowing_corpus_count
2023,Stefan Hegselmann1stefan.hegselmann@uni-muenster.de,The cleaned title is: Deep Learning for Image Classification Tasks - A Comprehensive Review.,"Stefan Hegselmann1stefan.hegselmann@uni-muenster.de, Antonio Parziale2anparziale@unisa.it, Divya Shanmugam3divyas@mit.edu, Shengpu Tang3shengpu.tang@gmail.com, Kristen Severson4kseverson@microsoft.com, Mercy Nyamewaa Asiedu5masiedu@google.com, Serina Chang5serinac@stanford.edu, Bonaventure F. P. Dossou6bonaventure.dossou@mila.quebec, Qian Huang6qhwang@cs.stanford.edu, Fahad Kamran7fhdkmrn@umich.edu, Haoran Zhang7haoranz@mit.edu, Sujay Nagaraj8s.nagaraj@mail.utoronto.ca, Luis Oala9luis.oala@dotphoton.com, Shan Xu9xushan@caict.ac.cn, Chinasa T. Okolo10chinasa@cs.cornell.edu, Helen Zhou10hlzhou@andrew.cmu.edu","stefan.hegselmann@uni-muenster.de
anparziale@unisa.it
divyas@mit.edu
shengpu.tang@gmail.com
kseverson@microsoft.com
masiedu@google.com
serinac@stanford.edu
bonaventure.dossou@mila.quebec
qhwang@cs.stanford.edu
fhdkmrn@umich.edu
haoranz@mit.edu
s.nagaraj@mail.utoronto.ca
luis.oala@dotphoton.com
xushan@caict.ac.cn
chinasa@cs.cornell.edu
hlzhou@andrew.cmu.edu","submissions and 198 full submissions (Pro- ceedings and Findings track) to the ML4H sympo- sium, constituting an over 40% increase in submis- sion volume compared to the prior year (Parziale et al., 2022). This underscores the growing respect for ML4H as a venue to discuss state-of-the-art tech- nology at the intersection of machine learning and health. The program committee consisted of 19 area chairs and 182 reviewers who completed a total of 505 re- views. Each Proceedings submission received at least three high-quality reviews, while each Findings sub- mission received at least two. The sponsors had no influence on the review process. Out of the 136 submissions to the Proceedings track, 39 were accepted to appear in the ML4H 2023 proceedings (acceptance rate of 28.7%). We allowed reviewers to recommend transfers of papers from the Proceedings track to the non-archival Findings track. In total 31 Proceedings submissions were accepted to the Findings track (22.8% transfer rate). Out of the 76 submissions to the Findings track, 40 were accepted (acceptance rate of 52.6%). As a result, there were 71 submissions accepted to the Findings track. The Findings paper were given the oppor- tunity to be included in an ML4H arXiv index at https://arxiv.org/abs/2312.00655 . 2.3. Paper Awards The General Chairs and Program Committee (Chairs and Subchairs) worked together to select the paper awards. Our awards consisted of: •Best Proceedings paper (one winner and one honorable mention): any paper accepted in the Proceedings track was eligible,•Best Findings paper (one winner and one honor- able mention): any paper accepted in the Find- ings track was eligible, •Best Newcomer paper: any paper where the first author was submitting to ML4H for the first time was eligible, •Best Thematic paper(s): any paper that was rel- evant to the themes was eligible. For the Genera- tive AI for Health theme, we included any paper where the authors chose the topic “Generative AI” as one of thei",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2023,Towards Equitable Kidney Tumor Segmentation: Bias Evaluationand Mitigation,Towards Equitable Kidney Tumor Segmentation: Bias Evaluation and Mitigation,"Muhammad Muneeb Afzal muneeb.afzal@nyu.edu, Muhammad Osama Khan osama.khan@nyu.edu, Shujaat Mirza shujaat.mirza@nyu.edu, New York University, New York, USA","muneeb.afzal@nyu.edu
osama.khan@nyu.edu
shujaat.mirza@nyu.edu","Kidney tumors, affecting over 400,000 individ- uals annually, require accurate segmentation for effective treatment and surgical planning. Yet, manual segmentation is time-consuming, steering the medical community towards auto- mated methods. While computer-aided diag- nostic tools promise improvements, their tran- sition into the real world mandates an under- standing of their performance across diverse population subgroups. Our study is the first to investigate fairness concerning kidney and tu- mor segmentation, particularly focusing on sen- sitive attributes like sex and age. Our findings show an existence of bias in performance across both attributes. In particular, despite a male- dominated training dataset, females showed su- perior segmentation performance. Age groups 60-70 and above 70 also deviated significantly from the average performance for all ages. To address these biases, we comprehensively ex- plore bias mitigation strategies - encompass- ing pre-processing techniques (Resampling Al- gorithm and Stratified Batch Sampling) and in- processing methods (Fair Meta-learning and ar- chitectural adjustments). Specifically, Atten- tion U-Net was identified as the optimal model for balancing fairness across both attributes while maintaining high segmentation perfor- mance. We present a crucial insight that the ar- chitecture itself could be a source of inherent bi- ases, and careful selection of the network design can inherently reduce these biases. Our assess- ment of UNet variants challenges the prevailing paradigm of model selection predicated solely on segmentation performance, especially con- sidering the profound implications biases can have in clinical outcomes. Keywords: Fair AI, Segmentation, Kidney Tumors, Bias Mitigation1.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2023,"Representingvisualclassificationasalinearcombinationofwords Shobhit Agarwalshobhitagarwal122@gmail.com Dana- Farber Cancer Institute, Boston, M A Reedy High School, Frisco, T X",Representing Visual Classification as a Linear Combination of Words.,"Shobhit Agarwal shobhitagarwal122@gmail.com, Dana-Farber Cancer Institute, Boston, MA, Reedy High School, Frisco, TX, Yevgeniy R. Semenov ysemenov@mgh.harvard.edu, Massachusetts General Hospital, Boston, MA, Harvard Medical School, Boston, MA, William Lotter∗lotterb@ds.dfci.harvard.edu, Dana-Farber Cancer Institute, Boston, MA, Brigham and Women’s Hospital, Boston, MA, Harvard Medical School, Boston, MA","shobhitagarwal122@gmail.com
ysemenov@mgh.harvard.edu
lotterb@ds.dfci.harvard.edu","Explainability is a longstanding challenge in deep learning, especially in high-stakes domains like healthcare. Common explainability meth- ods highlight image regions that drive an AI model’s decision. Humans, however, heavily rely on language to convey explanations of not only “where” but “what”. Additionally, most explainability approaches focus on explaining individual AI predictions, rather than describ- ing the features used by an AI model in gen- eral. The latter would be especially useful for model and dataset auditing, and potentially even knowledge generation as AI is increasingly being used in novel tasks. Here, we present an explainability strategy that uses a vision- language model to identify language-based de- scriptors of a visual classification task. By leveraging a pre-trained joint embedding space between images and text, our approach esti- mates a new classification task as a linear com- bination of words, resulting in a weight for each word that indicates its alignment with the vision-based classifier. We assess our approach using two medical imaging classification tasks, where we find that the resulting descriptors largely align with clinical knowledge despite a lack of domain-specific language training. How- ever, our approach also identifies the potential for ‘shortcut connections’ in the public datasets used. Towards a functional measure of explain- ability, we perform a pilot reader study where we find that the AI-identified words can en- able non-expert humans to perform a special- ized medical task at a non-trivial level. Alto- ∗Corresponding authorgether, our results emphasize the potential of using multimodal foundational models to de- liver intuitive, language-based explanations of visual tasks. Keywords: Explainability, vision-language models, HCI, dataset auditing",1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2023,Learning Temporal Higher-order Patternsto Detect Anomalous Brain Activity,Learning Temporal Higher-Order Patterns to Detect Anomalous Brain Activity.,"Ali Behrouz ab2947@cornell.edu, Cornell University, NY, USA, Farnoosh Hashemi sh2574@cornell.edu, Cornell University, NY, USA","ab2947@cornell.edu
sh2574@cornell.edu","Due to recent advances in machine learning on graphs, representing the connections of the hu- man brain as a network has become one of the most pervasive analytical paradigms. However, most existing graph machine learning-based methods suffer from a subset of five critical limitations: They are (1) designed for simple pair-wise interactions while recent studies on the human brain show the existence of higher-order dependencies of brain regions, (2) designed to perform on pre-constructed networks from time- series data, which limits their generalizability, (3) designed for classifying brain networks, lim- iting their ability to reveal underlying patterns that might cause the symptoms of a disease or disorder, (4) designed for learning of static patterns, missing the dynamics of human brain activity, and (5) designed in supervised setting, relying their performance on the existence of labeled data. To address these limitations, we present HADiB , an end-to-end anomaly detec- tion model that automatically learns the struc- ture of the hypergraph representation of the brain from neuroimage data. HADiB uses a tetra-stage message-passing mechanism along with an attention mechanism that learns the im- portance of higher-order dependencies of brain regions. We further present a new adaptive hypergraph pooling to obtain brain-level repre- sentation, enabling HADiB to detect the neu- roimage of people living with a specific disease or disorder. Our experiments on Parkinson’s Disease, Attention Deficit Hyperactivity Disor- der, and Autism Spectrum Disorder show the efficiency and effectiveness of our approaches in detecting anomalous brain activity. Keywords: Brain Networks, Temporal Hyper- graphs, Anomaly Detection1.",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2023,Multi-modal Graph Learningover U M L S Knowledge Graphs,Multi-modal Graph Learning over UMLS Knowledge Graphs,"Manuel Burger manuel.burger@inf.ethz.ch, Gunnar R¨ atsch raetsch@inf.ethz.ch, Rita Kuznetsova rita.kuznetsova@inf.ethz.ch, Department of Computer Science, ETH Z¨ urich, Switzerland","manuel.burger@inf.ethz.ch
raetsch@inf.ethz.ch
rita.kuznetsova@inf.ethz.ch","Clinicians are increasingly looking towards ma- chine learning to gain insights about patient progression. We propose a novel approach named Multi-Modal UMLS Graph Learning (MMUGL) for learning meaningful representa- tions of medical concepts using graph neural networks over knowledge graphs based on the unified medical language system. These con- cept representations are aggregated to repre- sent a patient visit and then fed into a sequence model to perform predictions at the granularity of multiple hospital visits of a patient. We im- prove performance by incorporating prior med- ical knowledge and considering multiple modal- ities. We compare our method to existing ar- chitectures proposed to learn representations at different granularities on the MIMIC-III dataset and show that our approach outperforms these methods. The results demonstrate the signifi- cance of multi-modal medical concept represen- tations based on prior medical knowledge. We provide our code on GitHub1. Keywords: Knowledge Graphs, EHR, UMLS, Multi-Modal",1,0,0,2,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
