{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NaN Citation Counts:\n",
      "CHIL: 29\n",
      "ML4H: 16\n",
      "MLHC: 110\n",
      "Total: 155\n",
      "Percentage: 24.41%\n",
      "\n",
      "Uncleaned Title Counts:\n",
      "CHIL: 4\n",
      "ML4H: 3\n",
      "MLHC: 23\n",
      "Total: 30\n",
      "Percentage: 4.72%\n",
      "\n",
      "NaN Title Counts:\n",
      "CHIL: 4\n",
      "ML4H: 3\n",
      "MLHC: 23\n",
      "Total: 30\n",
      "Percentage: 4.72%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def count_nan_citations(chil_df: pd.DataFrame, ml4h_df: pd.DataFrame, mlhc_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Counts the number of rows where the 'citation_count' column is NaN for each dataframe.\n",
    "    \n",
    "    Args:\n",
    "        chil_df, ml4h_df, mlhc_df: pandas DataFrames containing paper data\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with the counts of NaN citations for each conference\n",
    "    \"\"\"\n",
    "    nan_citations = {\n",
    "        'CHIL': chil_df['citation_count'].isna().sum(),\n",
    "        'ML4H': ml4h_df['citation_count'].isna().sum(),\n",
    "        'MLHC': mlhc_df['citation_count'].isna().sum()\n",
    "    }\n",
    "    \n",
    "    # Calculate totals and percentage\n",
    "    total_nan_citations = sum(nan_citations.values())\n",
    "    total_papers = len(chil_df) + len(ml4h_df) + len(mlhc_df)\n",
    "    \n",
    "    nan_citations['Total'] = total_nan_citations\n",
    "    nan_citations['Percentage'] = (total_nan_citations / total_papers) * 100\n",
    "    \n",
    "    return nan_citations\n",
    "\n",
    "def count_uncleaned_titles(chil_df: pd.DataFrame, ml4h_df: pd.DataFrame, mlhc_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Counts the number of rows where the 'cleaned_title' column is empty or NaN for each dataframe.\n",
    "    \n",
    "    Args:\n",
    "        chil_df, ml4h_df, mlhc_df: pandas DataFrames containing paper data\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with the counts of uncleaned titles for each conference\n",
    "    \"\"\"\n",
    "    # Function to count empty or NaN titles\n",
    "    def count_empty_or_nan(df):\n",
    "        if 'cleaned_title' not in df.columns:\n",
    "            return len(df)  # If column doesn't exist, count all rows as uncleaned\n",
    "        return df['cleaned_title'].isna().sum() + (df['cleaned_title'] == '').sum()\n",
    "    \n",
    "    uncleaned_titles = {\n",
    "        'CHIL': count_empty_or_nan(chil_df),\n",
    "        'ML4H': count_empty_or_nan(ml4h_df),\n",
    "        'MLHC': count_empty_or_nan(mlhc_df)\n",
    "    }\n",
    "    \n",
    "    # Calculate totals and percentage\n",
    "    total_uncleaned = sum(uncleaned_titles.values())\n",
    "    total_papers = len(chil_df) + len(ml4h_df) + len(mlhc_df)\n",
    "    \n",
    "    uncleaned_titles['Total'] = total_uncleaned\n",
    "    uncleaned_titles['Percentage'] = (total_uncleaned / total_papers) * 100\n",
    "    \n",
    "    return uncleaned_titles\n",
    "\n",
    "def count_nan_titles(chil_df, ml4h_df, mlhc_df):\n",
    "    \"\"\"\n",
    "    Counts the number of rows where the 'title' column is NaN for each dataframe.\n",
    "    \n",
    "    Args:\n",
    "    chil_df, ml4h_df, mlhc_df: pandas DataFrames containing paper data\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with the counts of NaN titles for each conference\n",
    "    \"\"\"\n",
    "    nan_titles = {\n",
    "        'CHIL': chil_df['title'].isna().sum(),\n",
    "        'ML4H': ml4h_df['title'].isna().sum(),\n",
    "        'MLHC': mlhc_df['title'].isna().sum()\n",
    "    }\n",
    "    \n",
    "    total_nan_titles = sum(nan_titles.values())\n",
    "    total_papers = len(chil_df) + len(ml4h_df) + len(mlhc_df)\n",
    "    \n",
    "    nan_titles['Total'] = total_nan_titles\n",
    "    nan_titles['Percentage'] = (total_nan_titles / total_papers) * 100\n",
    "    \n",
    "    return nan_titles\n",
    "\n",
    "file_paths = {\n",
    "            'ml4h': {\n",
    "                'input': 'data/cleaned/ml4h/ml4h_cleaned.csv',\n",
    "                'output': 'data/processed/ml4h/ml4h_citations.csv'\n",
    "            },\n",
    "            'chil': {\n",
    "                'input': 'data/cleaned/chil/chil_cleaned.csv',\n",
    "                'output': 'data/processed/chil/chil_citations.csv'\n",
    "            },\n",
    "            'mlhc': {\n",
    "                'input': 'data/cleaned/mlhc/mlhc_cleaned.csv',\n",
    "                'output': 'data/processed/mlhc/mlhc_citations.csv'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Load the dataframes\n",
    "chil_semantic_df = pd.read_csv(file_paths['chil']['output'])\n",
    "ml4h_semantic_df = pd.read_csv(file_paths['ml4h']['output'])\n",
    "mlhc_semantic_df = pd.read_csv(file_paths['mlhc']['output'])\n",
    "\n",
    "# Count NaN citations\n",
    "nan_citation_counts = count_nan_citations(chil_semantic_df, ml4h_semantic_df, mlhc_semantic_df)\n",
    "\n",
    "# Print the NaN citation results\n",
    "print(\"\\nNaN Citation Counts:\")\n",
    "for conference, count in nan_citation_counts.items():\n",
    "    if conference == 'Percentage':\n",
    "        print(f\"{conference}: {count:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{conference}: {count}\")\n",
    "\n",
    "# Count uncleaned titles\n",
    "uncleaned_title_counts = count_uncleaned_titles(chil_semantic_df, ml4h_semantic_df, mlhc_semantic_df)\n",
    "\n",
    "# Print the uncleaned title results\n",
    "print(\"\\nUncleaned Title Counts:\")\n",
    "for conference, count in uncleaned_title_counts.items():\n",
    "    if conference == 'Percentage':\n",
    "        print(f\"{conference}: {count:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{conference}: {count}\")\n",
    "\n",
    "# Count NaN titles\n",
    "nan_title_counts = count_nan_titles(chil_semantic_df, ml4h_semantic_df, mlhc_semantic_df)\n",
    "\n",
    "# Print the NaN title results\n",
    "print(\"\\nNaN Title Counts:\")\n",
    "for conference, count in nan_title_counts.items():\n",
    "    if conference == 'Percentage':\n",
    "        print(f\"{conference}: {count:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{conference}: {count}\")\n",
    "\n",
    "# ... [Rest of the code remains unchanged] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Dataframe Sizes:\n",
      "CHIL: 155\n",
      "ML4H: 134\n",
      "MLHC: 346\n",
      "\n",
      "NaN Title Counts (Before Removal):\n",
      "CHIL: 4\n",
      "ML4H: 3\n",
      "MLHC: 23\n",
      "Total: 30\n",
      "Percentage: 4.72%\n",
      "\n",
      "Dataframe Sizes After Removing NaN Titles:\n",
      "CHIL: 151\n",
      "ML4H: 131\n",
      "MLHC: 323\n",
      "\n",
      "NaN Citation Counts (After Removing NaN Titles):\n",
      "CHIL: 25\n",
      "ML4H: 13\n",
      "MLHC: 87\n",
      "Total: 125\n",
      "Percentage: 20.66%\n",
      "\n",
      "Cleaned CSV files have been saved in the 'processed_data/cleaned' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# ... [Previous functions remain unchanged] ...\n",
    "\n",
    "# # Load the dataframes\n",
    "# chil_semantic_df = pd.read_csv(\"processed_data/chil_semantic_scholar_citations.csv\")\n",
    "# ml4h_semantic_df = pd.read_csv(\"processed_data/ml4h_semantic_scholar_citations.csv\")\n",
    "# mlhc_semantic_df = pd.read_csv(\"processed_data/mlhc_semantic_scholar_citations.csv\")\n",
    "\n",
    "# Print original dataframe sizes\n",
    "print(\"\\nOriginal Dataframe Sizes:\")\n",
    "print(f\"CHIL: {len(chil_semantic_df)}\")\n",
    "print(f\"ML4H: {len(ml4h_semantic_df)}\")\n",
    "print(f\"MLHC: {len(mlhc_semantic_df)}\")\n",
    "\n",
    "# Count NaN titles before removal\n",
    "nan_title_counts = count_nan_titles(chil_semantic_df, ml4h_semantic_df, mlhc_semantic_df)\n",
    "\n",
    "# Print the NaN title results before removal\n",
    "print(\"\\nNaN Title Counts (Before Removal):\")\n",
    "for conference, count in nan_title_counts.items():\n",
    "    if conference == 'Percentage':\n",
    "        print(f\"{conference}: {count:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{conference}: {count}\")\n",
    "\n",
    "# Remove rows with NaN titles\n",
    "chil_semantic_df = chil_semantic_df.dropna(subset=['title'])\n",
    "ml4h_semantic_df = ml4h_semantic_df.dropna(subset=['title'])\n",
    "mlhc_semantic_df = mlhc_semantic_df.dropna(subset=['title'])\n",
    "\n",
    "# Print new dataframe sizes\n",
    "print(\"\\nDataframe Sizes After Removing NaN Titles:\")\n",
    "print(f\"CHIL: {len(chil_semantic_df)}\")\n",
    "print(f\"ML4H: {len(ml4h_semantic_df)}\")\n",
    "print(f\"MLHC: {len(mlhc_semantic_df)}\")\n",
    "\n",
    "# Recount NaN citations after removing rows with NaN titles\n",
    "nan_citation_counts = count_nan_citations(chil_semantic_df, ml4h_semantic_df, mlhc_semantic_df)\n",
    "\n",
    "# Print the new NaN citation results\n",
    "print(\"\\nNaN Citation Counts (After Removing NaN Titles):\")\n",
    "for conference, count in nan_citation_counts.items():\n",
    "    if conference == 'Percentage':\n",
    "        print(f\"{conference}: {count:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{conference}: {count}\")\n",
    "\n",
    "# Save cleaned dataframes to new CSV files\n",
    "output_dir = \"data/cleaned\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "chil_semantic_df.to_csv(file_paths['chil']['output'], index=False)\n",
    "ml4h_semantic_df.to_csv(file_paths['ml4h']['output'], index=False)\n",
    "mlhc_semantic_df.to_csv(file_paths['mlhc']['output'], index=False)\n",
    "\n",
    "print(\"\\nCleaned CSV files have been saved in the 'data/processed' directory.\")\n",
    "\n",
    "# ... [Rest of the code remains unchanged] ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the important titles we want to query SerpAPI for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Titles with NaN citations:\n",
      "\n",
      "CHIL (23):\n",
      "- MIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for\n",
      "- Visual Che Xbert: Addressing the Discrepancy Between Radiology Report Labels and Image Labels\n",
      "- Che X-Transfer: Performance and Parameter Efficiency of Image Net Models for Chest X-Ray Interpretation\n",
      "- Interpretable Missing Values in Healthcare: Figure 7 - Impact of Father's Education on Infant Mortality Risk. Appendix A: Testing for MCAR with EBM: Case Study.\n",
      "- Toward the Practical Utility of Federated Learning in the Medical Domain\n",
      "- Evaluating Model Performance in Medical Datasets Over Time: A Snapshot into the State of Proceedings and the First 20 Papers That Came Up in the Radiology Medical Journal When Searching for the Keyword \"Machine Learning\" and Filtering for Papers from To\n",
      "- Understanding and Predicting Environment Effects on Individuals with T2D: Appendix A - CGM Dataset. We include information on the range of values for each external factor in our dataset in Table 4. Note that all extreme weather events and all temporal events are binary-valued. We plot the distribution of the number of days recorded for each individual in Figure 4. The large spike in the last bucket contains roughly 10% of individuals in our cohort and represents people who have recorded M data on over 75% of days over the 2.5-year duration of data collection. Figure 4: Distribution of Number of Recorded Days of M Data per Individual. The spike in the last bucket is due to about 10% of individuals providing recordings (nearly) every day over the 2.5-year data collection period. B. Classifier Details: Information on the values input into the random forest classifiers is provided in Table 5. Note that the M and activity data is not used for classifiers, thus no M activity data features. C. Additional Results: Here, we provide plots showing the PR-AUC performance of our classifiers. We observe similar results as described in Section 3.2. In Figure 5, we plot the C of models across 8\n",
      "- Enhancing Collaborative Medical Outcomes through Private Synthetic Hypercube Augmentation: PriSHA\n",
      "- Improved Bayesian Permutation Entropy Estimator\n",
      "- Regularizing and Interpreting Vision Transformers by Patch Selection on Echocardiography Data\n",
      "- From Basic to Extra Features: Hypergraph Transformer Pre-Training Then Fine-Tuning for Balanced Clinical Predictions on EHR\n",
      "- Explainable and Privacy-Preserving Machine Learning via Domain-Aware Symbolic Regression\n",
      "- Simulation of Health Time Series with Nonstationarity\n",
      "- Brain-Mamba: Encoding Brain Activity via Selective State Space Models\n",
      "- Addressing Wearable Sleep Tracking Inequity: A New Dataset and Novel Methods for a Population with Sleep Disorders\n",
      "- FETCH: A Fast and Efficient Technique for Channel Selection in G Wearable Systems\n",
      "- Using Expert Gaze for Self-Supervised and Supervised Contrastive Learning of Glaucoma from OCT Data\n",
      "- Scalable Subtype and Stage Inference via Simultaneous Clustering of Subjects and Biomarkers\n",
      "- Retrieving Evidence from EHRs with LMs: Possibilities and Challenges\n",
      "- Development of Error Passing Network for Optimizing the Prediction of VO2 Peak in Acute Leukemia Survivors\n",
      "- Dose Mate: A Real-World Evaluation of Machine Learning Classification of Pill Taking Using Wrist-Worn Motion Sensors\n",
      "- Systematic Evaluation of Self-Supervised Learning Approaches for Wearable-Based Fatigue Recognition\n",
      "- Across-Study Analysis of Wearable Datasets and the Generalizability of Acute Illness Monitoring Models\n",
      "\n",
      "ML4H (9):\n",
      "- Che X-Photo: 10,000+ Photos and Transformations of Chest X-Rays for Benchmarking Deep Learning Robustness\n",
      "- Front Matter\n",
      "- End-to-End Sequential Sampling and Reconstruction for MRI\n",
      "- An Extensive Data Processing Pipeline for MIC-V\n",
      "- Automated L O I N C should be formatted as LOINC, and \"Pre-trained\" should be hyphenated as \"Pre-trained\" is correct in this context. Here is the cleaned title:\n",
      "\n",
      "Automated LOINC Standardization Using Pre-trained Large Language Models\n",
      "- Adapting Pre-Trained Vision Transformers from 2D to 3D Improves Segmentation: Details of the Experimental Settings. We Do Not Perform Heavy Hyperparameter Tuning to Ensure the Generalizability of Our Best Practices. The Ensemble Is Not Used, Which Can Further Improve Performances. For Brain, Vessel, and Pancreas, We Train Each Model with 250,000 Steps Given Their Larger Training Set. We Provide All the Codes, Including Data Pre-Processing, Data Loading, Model Training, and Evaluation at https://github.com/yuhui-zh15/ Trans Seg Hyperparam Value Batch Size 16 Patch Size\n",
      "- L-Ms Accelerate Annotation for Medical Information Extraction\n",
      "- Multimodal Pre-Training of Medical Time Series and Notes\n",
      "- Robust Semi-Supervised Segmentation with Time-Step Ensemble Blending Diffusion Models\n",
      "\n",
      "MLHC (60):\n",
      "- Stanford University - Machine Learning for Healthcare - GPT-17/Hyphen-Case-18\n",
      "- DEPSINE: Automated Omic Data Integration\n",
      "- Deep Survival Analysis: Nonparametrics and Missingness. Clinical Career Requires Understanding the Time to Medical Events. Medical Events Include the Time to a Disease Like Chronic Kidney Disease Progressing or the Time to a Complication as in Stroke for High Blood Pressure. Models for Event Times Live in the Framework Provided by Survival Analysis.\n",
      "- Michigan Genomics Initiative Cohort: A Shared Resource for Accelerating Research in Precision Health. Overview: As Part of the Community Data Challenge, We Are Making Available a Subset of Data from the Initiative (MGI) to Researchers. The Database Will Be Shared with Attendees Through Google Cloud Platform (CP) via Big Query, Allowing Users to Explore the Data, Build, and Validate Machine Learning Models Directly on P. During the Day of the Challenge, Participants Will Work in Interdisciplinary Teams to Articulate a Technically Interesting, Clinically Relevant Problem That Could Be Solved Using the Data. The Goal Is Not to Solve the Problem During the Event, but to Start Formulating the Problem. Problems Will Be Judged by a Panel of Experts, and Winning Teams Will Receive Up to $1,000 in P Credits That Can Be Applied to the Environment Throughout the Following Year to Tackle the Proposed Problem.\n",
      "- EEG to Text: Learning to Write Medical Reports from EEG Recordings\n",
      "- Using Predictive Mortality and Cardiogenic Shock Identification Tools to Support Team-Based Treatment Intervention on Adult Cardiology Patients at Duke University Hospital\n",
      "- Examining the Measurement of Quality in Healthcare Using Artificial Intelligence Methods: A Study of Quality in Long-Term Care\n",
      "- Clinical Collaboration Sheets: 53 Questions to Guide a Collaboration\n",
      "- Che Xpert++: Approximating the Xpert Labeler for Speed, Differentiability, and Probabilistic Output\n",
      "- Deep Learning Approach for Autonomous Medical Diagnosis in Spanish Language\n",
      "- Clinical Abstract Topic Modeling of Patient Portal and Telephone Encounter Messages: Insights from a Cardiology Practice\n",
      "- Development of Phenotype Algorithms for Common Acute Conditions Using SHapley Additive Ex-Planation Values\n",
      "- Predicting Cardiac Decompensation and Cardiogenic Shock Phenotypes for Duke University Hospital Patients\n",
      "- I-C Unity: A Software Tool to Harmonize the M-C-I and Amsterdam U-C Databases\n",
      "- Prediction of Critical Pediatric Perioperative Adverse Events Using the APRICOT Dataset\n",
      "- A Heart Rate Algorithm to Predict High-Risk Children Presenting to the Pediatric Emergency Department\n",
      "- Machine Learning to Automate Clinician-Designed Empirical Manual for Congenital Heart Disease Identification in Large Claims Database\n",
      "- Deep Learning Airway Structure Identification for Video Intubation\n",
      "- Denoising Stimulated Raman Histology Using Weak Supervision to Improve Label-Free Optical Microscopy of Human Brain Tumors\n",
      "- Journal of Machine Learning Research 149: 1–29, Deep Generative Analysis for Task-Based Functional MRI Experiments\n",
      "- Power-Constrained Bandits\n",
      "- MIC-SBDH: A Dataset for Social and Behavioral Determinants of Health\n",
      "- Point Processes for Competing Observations with Recurrent Networks (POCRN): A Generative Model of EH Data\n",
      "- Audi Face: Multimodal Deep Learning for Depression Screening\n",
      "- Temporal Patterns of Primary Care Utilization as Predictors for ICU Admission\n",
      "- Cycl Ops: A Unified Framework for Data Extraction and Rigorous Evaluation of Machine Learning Models for Clinical Use Cases\n",
      "- Integration of a Post-Operative Opioid Calculator into an Academic Gynecologic Surgery Practice\n",
      "- STANDIG: Together Standards for Data Diversity, Inclusivity, and Generalisability\n",
      "- Med-BER Tv2: Clinical Foundation Model on Standardized Secondary Clinical Data\n",
      "- A Machine Learning-Based Approach to Classifying a Provider's Description of Chest Pain\n",
      "- Patient and Room Activity Video Summary (PRAS) in the ICU: Rapidly Interpretable, ML-Generated Clinical Summaries of the Overnight Period\n",
      "- Single-Cell Phenotyping Using Optical Imaging and Artificial Intelligence\n",
      "- Transparent and Distributed AI Prediction Modeling: Case Study on Pediatric COVID\n",
      "- Predictive Models for Clopidogrel Outcomes Using Prescription Records and Diagnosis Codes\n",
      "- Interpretable (Not Just Post-Hoc Explainable) Heterogeneous Survivors' Bias-Corrected Treatment Effects for Assignment of Post-Discharge Interventions to Prevent Readmissions\n",
      "- Typed Markers and Context for Clinical Temporal Relation Extraction\n",
      "- Learning False Sections in Medical Conversations: Iterative Pseudo-Labeling and Human-in-the-Loop Approach\n",
      "- Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision\n",
      "- TIER: Text-Image Entropy Regularization for Medical CLP-Style Models\n",
      "- PrivECG: Generating Private G for End-to-End Anonymization\n",
      "- Rad Graph 2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction\n",
      "- Use of Machine Learning Techniques for Phenotyping Ischemic Stroke Instead of the Rule-Based Methods: A Nation-Wide Population-Based Study\n",
      "- Early Identification of the Need for CRT in Critically Ill Children: A Machine Learning Approach\n",
      "- Development of a Dataset and Prospective Evaluation of a Model to Identify High-Quality Papers on the Clinical Impact of Pharmacist Interventions\n",
      "- Social Determinants of Health Documented in Clinical Notes Improve Hospital Prediction in Home Health Care\n",
      "- Developing a Patient Similarity Network for Predicting Post-Stroke Urinary Tract Infection Risk in Hospitalized Immobile Patients\n",
      "- Examining the Ability of Different ML Approaches to Predict Health Outcomes with Digital Platform\n",
      "- A Counterfactual-Based Approach for Interpreting Deep Learning Models in Electrocardiogram Analysis\n",
      "- Natural Language Processing for Automated Extraction of Breast Cancer Information for the Registry. National Cancer Registries Rely on Manual Abstraction of Free-Text Clinical Records to Collect\n",
      "- Expanding Composite Disease Labels Improves ECG Deep Learning Model Performance for Structural Heart Detection\n",
      "- Using Natural Language Processing to Predict Workings of Sources of Infection\n",
      "- Guidance Tool for Development and Implementation of Safe Healthcare AI\n",
      "- Enhancing Deep Learning in Detecting Acute Myocardial Infarction via Anatomically Informed 12-Lead ECG\n",
      "- Leveraging Data Science for Optimal Follow-up of Multimorbidity Patients - A Research Protocol\n",
      "- Predicting Behavioral Emergencies in the Hospital\n",
      "- Development of a Machine Learning Classification Model for ICU Admission Following Resuscitation at a Level Trauma Center\n",
      "- Examining Unstructured Free-Text Data from Publicly Accessible Data Streams Gathered from Communities Confronting a Public Health Crisis: A Study on the Predictive Value of Advanced Natural Language Processing-Based Techniques in Real-Time Pandemic Handling\n",
      "- A Machine Learning Model Using In-Game Data for Predicting Unhealthy Substance Use Among Adolescents\n",
      "- Antihypertensive Drug Repurposing for Dementia Prevention: Target Trial Emulations in Two Large-Scale Electronic Health Record Systems\n",
      "- Not So Black and White: Confounders Mediate AI Prediction of Race on Chest X-Rays\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# ... [Previous functions remain unchanged] ...\n",
    "\n",
    "def get_titles_with_nan_citations(df):\n",
    "    \"\"\"\n",
    "    Returns a list of cleaned titles for papers with NaN citation counts.\n",
    "    \n",
    "    Args:\n",
    "    df: pandas DataFrame containing paper data\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of cleaned titles for papers with NaN citation counts\n",
    "    \"\"\"\n",
    "    return df[df['citation_count'].isna()]['cleaned_title'].tolist()\n",
    "\n",
    "# Load the cleaned dataframes\n",
    "# Get lists of titles with NaN citations for each conference\n",
    "chil_nan_citation_titles = get_titles_with_nan_citations(chil_semantic_df)\n",
    "ml4h_nan_citation_titles = get_titles_with_nan_citations(ml4h_semantic_df)\n",
    "mlhc_nan_citation_titles = get_titles_with_nan_citations(mlhc_semantic_df)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nTitles with NaN citations:\")\n",
    "print(f\"\\nCHIL ({len(chil_nan_citation_titles)}):\")\n",
    "for title in chil_nan_citation_titles:  # Print first 5 for brevity\n",
    "    print(f\"- {title}\")\n",
    "# if len(chil_nan_citation_titles) > 5:\n",
    "#     print(\"...\")\n",
    "\n",
    "print(f\"\\nML4H ({len(ml4h_nan_citation_titles)}):\")\n",
    "for title in ml4h_nan_citation_titles:  # Print first 5 for brevity\n",
    "    print(f\"- {title}\")\n",
    "# if len(ml4h_nan_citation_titles) > 5:\n",
    "#     print(\"...\")\n",
    "\n",
    "print(f\"\\nMLHC ({len(mlhc_nan_citation_titles)}):\")\n",
    "for title in mlhc_nan_citation_titles:  # Print first 5 for brevity\n",
    "    print(f\"- {title}\")\n",
    "# if len(mlhc_nan_citation_titles) > 5:\n",
    "#     print(\"...\")\n",
    "\n",
    "# ... [Rest of the code remains unchanged] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying specific cleaning operations...\n",
      "\n",
      "Rows after specific cleaning:\n",
      "CHIL: 148\n",
      "ML4H: 127\n",
      "MLHC: 283\n",
      "\n",
      "Specifically cleaned CSV files have been saved in the 'processed_data/specifically_cleaned' directory.\n",
      "\n",
      "Number of titles containing 'preprint':\n",
      "CHIL: 0\n",
      "ML4H: 0\n",
      "MLHC: 0\n",
      "\n",
      "No CHIL titles contain 'preprint'.\n",
      "\n",
      "No ML4H titles contain 'preprint'.\n",
      "\n",
      "No MLHC titles contain 'preprint'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85908/566563017.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_title'] = df['cleaned_title'].replace({\n",
      "/tmp/ipykernel_85908/566563017.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_title'] = df['cleaned_title'].apply(lambda x: re.sub(r'Preprint: Under Review \\d+:\\s*\\d+–\\d+,', '', x).strip())\n",
      "/tmp/ipykernel_85908/566563017.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_title'] = df['cleaned_title'].apply(lambda x: re.sub(r'Preprint: Under Review \\d+–\\d+,?', '', x).strip())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ... [Previous functions remain unchanged] ...\n",
    "\n",
    "def specific_cleaning(df):\n",
    "    \"\"\"\n",
    "    Performs specific cleaning operations on the dataframe.\n",
    "    \n",
    "    Args:\n",
    "    df: pandas DataFrame containing paper data\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame: The cleaned dataframe\n",
    "    \"\"\"\n",
    "    # Titles to remove\n",
    "    titles_to_remove = [\n",
    "        \"Supplementary\",\n",
    "        \"Motivation for What Purpose Was the Dataset Created?\",\n",
    "        \"Conference Proceedings\",\n",
    "        \"Variable Total Age (Mean SD) 55.14\",\n",
    "        \"There is no title to clean\",\n",
    "        \"Machine Learning for Healthcare August 8-10\",\n",
    "        \"Development of a Clinical Decision Tool and Protocol for Identification and Treatment\",\n",
    "        \"Clinical Abstract Track\",\n",
    "        \"To 32.9 in 1,\"\n",
    "    ]\n",
    "    \n",
    "    # Remove rows with specified titles\n",
    "    pattern = '|'.join(map(re.escape, titles_to_remove))\n",
    "    df = df[~df['cleaned_title'].str.contains(pattern, case=False, regex=True)]\n",
    "    \n",
    "    # Clean specific titles\n",
    "    df['cleaned_title'] = df['cleaned_title'].replace({\n",
    "        \"EG to Text: Learning to Write Medical Reports from G Recordings\": \n",
    "        \"EEG to Text: Learning to Write Medical Reports from EEG Recordings\"\n",
    "    })\n",
    "    \n",
    "    # Remove \"Preprint: Under Review\" and associated numbers\n",
    "    df['cleaned_title'] = df['cleaned_title'].apply(lambda x: re.sub(r'Preprint: Under Review \\d+:\\s*\\d+–\\d+,', '', x).strip())\n",
    "    df['cleaned_title'] = df['cleaned_title'].apply(lambda x: re.sub(r'Preprint: Under Review \\d+–\\d+,?', '', x).strip())\n",
    "    \n",
    "    # Remove rows where the cleaned title is empty after cleaning\n",
    "    df = df[df['cleaned_title'] != '']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply specific cleaning to each dataframe\n",
    "print(\"Applying specific cleaning operations...\")\n",
    "chil_semantic_df = specific_cleaning(chil_semantic_df)\n",
    "ml4h_semantic_df = specific_cleaning(ml4h_semantic_df)\n",
    "mlhc_semantic_df = specific_cleaning(mlhc_semantic_df)\n",
    "\n",
    "# Print the number of rows in each dataframe after cleaning\n",
    "print(f\"\\nRows after specific cleaning:\")\n",
    "print(f\"CHIL: {len(chil_semantic_df)}\")\n",
    "print(f\"ML4H: {len(ml4h_semantic_df)}\")\n",
    "print(f\"MLHC: {len(mlhc_semantic_df)}\")\n",
    "\n",
    "# Save the specifically cleaned dataframes\n",
    "# output_dir = \"processed_data/specifically_cleaned\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "chil_semantic_df.to_csv(file_paths['chil']['output'], index=False)\n",
    "ml4h_semantic_df.to_csv(file_paths['ml4h']['output'], index=False)\n",
    "mlhc_semantic_df.to_csv(file_paths['mlhc']['output'], index=False)\n",
    "\n",
    "print(f\"\\nSpecifically cleaned CSV files have been saved in the '{output_dir}' directory.\")\n",
    "\n",
    "# Function to check for \"preprint\" in titles\n",
    "def count_preprint_titles(df):\n",
    "    return df['cleaned_title'].str.lower().str.contains('preprint').sum()\n",
    "\n",
    "# Count and print the number of titles containing \"preprint\"\n",
    "print(\"\\nNumber of titles containing 'preprint':\")\n",
    "print(f\"CHIL: {count_preprint_titles(chil_semantic_df)}\")\n",
    "print(f\"ML4H: {count_preprint_titles(ml4h_semantic_df)}\")\n",
    "print(f\"MLHC: {count_preprint_titles(mlhc_semantic_df)}\")\n",
    "\n",
    "# Function to get titles containing \"preprint\"\n",
    "def get_preprint_titles(df):\n",
    "    return df[df['cleaned_title'].str.lower().str.contains('preprint')]['cleaned_title'].tolist()\n",
    "\n",
    "# Print any titles that still contain \"preprint\"\n",
    "for name, df in [(\"CHIL\", chil_semantic_df), (\"ML4H\", ml4h_semantic_df), (\"MLHC\", mlhc_semantic_df)]:\n",
    "    preprint_titles = get_preprint_titles(df)\n",
    "    if preprint_titles:\n",
    "        print(f\"\\n{name} title(s) still containing 'preprint':\")\n",
    "        for title in preprint_titles:\n",
    "            print(f\"- {title}\")\n",
    "    else:\n",
    "        print(f\"\\nNo {name} titles contain 'preprint'.\")\n",
    "\n",
    "# ... [Rest of the code remains unchanged] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CHIL titles...\n",
      "Processing 23 titles with NaN citation counts...\n",
      "Saved results for: MIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for\n",
      "Saved results for: Visual Che Xbert: Addressing the Discrepancy Between Radiology Report Labels and Image Labels\n",
      "Saved results for: Che X-Transfer: Performance and Parameter Efficiency of Image Net Models for Chest X-Ray Interpretation\n",
      "Saved results for: Interpretable Missing Values in Healthcare: Figure 7 - Impact of Father's Education on Infant Mortality Risk. Appendix A: Testing for MCAR with EBM: Case Study.\n",
      "Saved results for: Toward the Practical Utility of Federated Learning in the Medical Domain\n",
      "Saved results for: Evaluating Model Performance in Medical Datasets Over Time: A Snapshot into the State of Proceedings and the First 20 Papers That Came Up in the Radiology Medical Journal When Searching for the Keyword \"Machine Learning\" and Filtering for Papers from To\n",
      "Saved results for: Understanding and Predicting Environment Effects on Individuals with T2D: Appendix A - CGM Dataset. We include information on the range of values for each external factor in our dataset in Table 4. Note that all extreme weather events and all temporal events are binary-valued. We plot the distribution of the number of days recorded for each individual in Figure 4. The large spike in the last bucket contains roughly 10% of individuals in our cohort and represents people who have recorded M data on over 75% of days over the 2.5-year duration of data collection. Figure 4: Distribution of Number of Recorded Days of M Data per Individual. The spike in the last bucket is due to about 10% of individuals providing recordings (nearly) every day over the 2.5-year data collection period. B. Classifier Details: Information on the values input into the random forest classifiers is provided in Table 5. Note that the M and activity data is not used for classifiers, thus no M activity data features. C. Additional Results: Here, we provide plots showing the PR-AUC performance of our classifiers. We observe similar results as described in Section 3.2. In Figure 5, we plot the C of models across 8\n",
      "Saved results for: Enhancing Collaborative Medical Outcomes through Private Synthetic Hypercube Augmentation: PriSHA\n",
      "Saved results for: Improved Bayesian Permutation Entropy Estimator\n",
      "Saved results for: Regularizing and Interpreting Vision Transformers by Patch Selection on Echocardiography Data\n",
      "Saved results for: From Basic to Extra Features: Hypergraph Transformer Pre-Training Then Fine-Tuning for Balanced Clinical Predictions on EHR\n",
      "Saved results for: Explainable and Privacy-Preserving Machine Learning via Domain-Aware Symbolic Regression\n",
      "Saved results for: Simulation of Health Time Series with Nonstationarity\n",
      "Saved results for: Brain-Mamba: Encoding Brain Activity via Selective State Space Models\n",
      "Saved results for: Addressing Wearable Sleep Tracking Inequity: A New Dataset and Novel Methods for a Population with Sleep Disorders\n",
      "Saved results for: FETCH: A Fast and Efficient Technique for Channel Selection in G Wearable Systems\n",
      "Saved results for: Using Expert Gaze for Self-Supervised and Supervised Contrastive Learning of Glaucoma from OCT Data\n",
      "Saved results for: Scalable Subtype and Stage Inference via Simultaneous Clustering of Subjects and Biomarkers\n",
      "Saved results for: Retrieving Evidence from EHRs with LMs: Possibilities and Challenges\n",
      "Saved results for: Development of Error Passing Network for Optimizing the Prediction of VO2 Peak in Acute Leukemia Survivors\n",
      "Saved results for: Dose Mate: A Real-World Evaluation of Machine Learning Classification of Pill Taking Using Wrist-Worn Motion Sensors\n",
      "Saved results for: Systematic Evaluation of Self-Supervised Learning Approaches for Wearable-Based Fatigue Recognition\n",
      "Saved results for: Across-Study Analysis of Wearable Datasets and the Generalizability of Acute Illness Monitoring Models\n",
      "Finished processing all titles with NaN citation counts.\n",
      "\n",
      "Processing ML4H titles...\n",
      "Processing 9 titles with NaN citation counts...\n",
      "Saved results for: Che X-Photo: 10,000+ Photos and Transformations of Chest X-Rays for Benchmarking Deep Learning Robustness\n",
      "Saved results for: Front Matter\n",
      "Saved results for: End-to-End Sequential Sampling and Reconstruction for MRI\n",
      "Saved results for: An Extensive Data Processing Pipeline for MIC-V\n",
      "Saved results for: Automated L O I N C should be formatted as LOINC, and \"Pre-trained\" should be hyphenated as \"Pre-trained\" is correct in this context. Here is the cleaned title:\n",
      "\n",
      "Automated LOINC Standardization Using Pre-trained Large Language Models\n",
      "Saved results for: Adapting Pre-Trained Vision Transformers from 2D to 3D Improves Segmentation: Details of the Experimental Settings. We Do Not Perform Heavy Hyperparameter Tuning to Ensure the Generalizability of Our Best Practices. The Ensemble Is Not Used, Which Can Further Improve Performances. For Brain, Vessel, and Pancreas, We Train Each Model with 250,000 Steps Given Their Larger Training Set. We Provide All the Codes, Including Data Pre-Processing, Data Loading, Model Training, and Evaluation at https://github.com/yuhui-zh15/ Trans Seg Hyperparam Value Batch Size 16 Patch Size\n",
      "Saved results for: L-Ms Accelerate Annotation for Medical Information Extraction\n",
      "Saved results for: Multimodal Pre-Training of Medical Time Series and Notes\n",
      "Saved results for: Robust Semi-Supervised Segmentation with Time-Step Ensemble Blending Diffusion Models\n",
      "Finished processing all titles with NaN citation counts.\n",
      "\n",
      "Processing MLHC titles...\n",
      "Processing 60 titles with NaN citation counts...\n",
      "Saved results for: Stanford University - Machine Learning for Healthcare - GPT-17/Hyphen-Case-18\n",
      "Saved results for: DEPSINE: Automated Omic Data Integration\n",
      "Saved results for: Deep Survival Analysis: Nonparametrics and Missingness. Clinical Career Requires Understanding the Time to Medical Events. Medical Events Include the Time to a Disease Like Chronic Kidney Disease Progressing or the Time to a Complication as in Stroke for High Blood Pressure. Models for Event Times Live in the Framework Provided by Survival Analysis.\n",
      "Saved results for: Michigan Genomics Initiative Cohort: A Shared Resource for Accelerating Research in Precision Health. Overview: As Part of the Community Data Challenge, We Are Making Available a Subset of Data from the Initiative (MGI) to Researchers. The Database Will Be Shared with Attendees Through Google Cloud Platform (CP) via Big Query, Allowing Users to Explore the Data, Build, and Validate Machine Learning Models Directly on P. During the Day of the Challenge, Participants Will Work in Interdisciplinary Teams to Articulate a Technically Interesting, Clinically Relevant Problem That Could Be Solved Using the Data. The Goal Is Not to Solve the Problem During the Event, but to Start Formulating the Problem. Problems Will Be Judged by a Panel of Experts, and Winning Teams Will Receive Up to $1,000 in P Credits That Can Be Applied to the Environment Throughout the Following Year to Tackle the Proposed Problem.\n",
      "Saved results for: EEG to Text: Learning to Write Medical Reports from EEG Recordings\n",
      "Saved results for: Using Predictive Mortality and Cardiogenic Shock Identification Tools to Support Team-Based Treatment Intervention on Adult Cardiology Patients at Duke University Hospital\n",
      "Saved results for: Examining the Measurement of Quality in Healthcare Using Artificial Intelligence Methods: A Study of Quality in Long-Term Care\n",
      "Saved results for: Clinical Collaboration Sheets: 53 Questions to Guide a Collaboration\n",
      "Saved results for: Che Xpert++: Approximating the Xpert Labeler for Speed, Differentiability, and Probabilistic Output\n",
      "Saved results for: Deep Learning Approach for Autonomous Medical Diagnosis in Spanish Language\n",
      "Saved results for: Clinical Abstract Topic Modeling of Patient Portal and Telephone Encounter Messages: Insights from a Cardiology Practice\n",
      "Saved results for: Development of Phenotype Algorithms for Common Acute Conditions Using SHapley Additive Ex-Planation Values\n",
      "Saved results for: Predicting Cardiac Decompensation and Cardiogenic Shock Phenotypes for Duke University Hospital Patients\n",
      "Saved results for: I-C Unity: A Software Tool to Harmonize the M-C-I and Amsterdam U-C Databases\n",
      "Saved results for: Prediction of Critical Pediatric Perioperative Adverse Events Using the APRICOT Dataset\n",
      "Saved results for: A Heart Rate Algorithm to Predict High-Risk Children Presenting to the Pediatric Emergency Department\n",
      "Saved results for: Machine Learning to Automate Clinician-Designed Empirical Manual for Congenital Heart Disease Identification in Large Claims Database\n",
      "Saved results for: Deep Learning Airway Structure Identification for Video Intubation\n",
      "Saved results for: Denoising Stimulated Raman Histology Using Weak Supervision to Improve Label-Free Optical Microscopy of Human Brain Tumors\n",
      "Saved results for: Journal of Machine Learning Research 149: 1–29, Deep Generative Analysis for Task-Based Functional MRI Experiments\n",
      "Saved results for: Power-Constrained Bandits\n",
      "Saved results for: MIC-SBDH: A Dataset for Social and Behavioral Determinants of Health\n",
      "Saved results for: Point Processes for Competing Observations with Recurrent Networks (POCRN): A Generative Model of EH Data\n",
      "Saved results for: Audi Face: Multimodal Deep Learning for Depression Screening\n",
      "Saved results for: Temporal Patterns of Primary Care Utilization as Predictors for ICU Admission\n",
      "Saved results for: Cycl Ops: A Unified Framework for Data Extraction and Rigorous Evaluation of Machine Learning Models for Clinical Use Cases\n",
      "Saved results for: Integration of a Post-Operative Opioid Calculator into an Academic Gynecologic Surgery Practice\n",
      "Saved results for: STANDIG: Together Standards for Data Diversity, Inclusivity, and Generalisability\n",
      "Saved results for: Med-BER Tv2: Clinical Foundation Model on Standardized Secondary Clinical Data\n",
      "Saved results for: A Machine Learning-Based Approach to Classifying a Provider's Description of Chest Pain\n",
      "Saved results for: Patient and Room Activity Video Summary (PRAS) in the ICU: Rapidly Interpretable, ML-Generated Clinical Summaries of the Overnight Period\n",
      "Saved results for: Single-Cell Phenotyping Using Optical Imaging and Artificial Intelligence\n",
      "Saved results for: Transparent and Distributed AI Prediction Modeling: Case Study on Pediatric COVID\n",
      "Saved results for: Predictive Models for Clopidogrel Outcomes Using Prescription Records and Diagnosis Codes\n",
      "Saved results for: Interpretable (Not Just Post-Hoc Explainable) Heterogeneous Survivors' Bias-Corrected Treatment Effects for Assignment of Post-Discharge Interventions to Prevent Readmissions\n",
      "Saved results for: Typed Markers and Context for Clinical Temporal Relation Extraction\n",
      "Saved results for: Learning False Sections in Medical Conversations: Iterative Pseudo-Labeling and Human-in-the-Loop Approach\n",
      "Saved results for: Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision\n",
      "Saved results for: TIER: Text-Image Entropy Regularization for Medical CLP-Style Models\n",
      "Saved results for: PrivECG: Generating Private G for End-to-End Anonymization\n",
      "Saved results for: Rad Graph 2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction\n",
      "Saved results for: Use of Machine Learning Techniques for Phenotyping Ischemic Stroke Instead of the Rule-Based Methods: A Nation-Wide Population-Based Study\n",
      "Saved results for: Early Identification of the Need for CRT in Critically Ill Children: A Machine Learning Approach\n",
      "Saved results for: Development of a Dataset and Prospective Evaluation of a Model to Identify High-Quality Papers on the Clinical Impact of Pharmacist Interventions\n",
      "Saved results for: Social Determinants of Health Documented in Clinical Notes Improve Hospital Prediction in Home Health Care\n",
      "Saved results for: Developing a Patient Similarity Network for Predicting Post-Stroke Urinary Tract Infection Risk in Hospitalized Immobile Patients\n",
      "Saved results for: Examining the Ability of Different ML Approaches to Predict Health Outcomes with Digital Platform\n",
      "Saved results for: A Counterfactual-Based Approach for Interpreting Deep Learning Models in Electrocardiogram Analysis\n",
      "Saved results for: Natural Language Processing for Automated Extraction of Breast Cancer Information for the Registry. National Cancer Registries Rely on Manual Abstraction of Free-Text Clinical Records to Collect\n",
      "Saved results for: Expanding Composite Disease Labels Improves ECG Deep Learning Model Performance for Structural Heart Detection\n",
      "Saved results for: Using Natural Language Processing to Predict Workings of Sources of Infection\n",
      "Saved results for: Guidance Tool for Development and Implementation of Safe Healthcare AI\n",
      "Saved results for: Enhancing Deep Learning in Detecting Acute Myocardial Infarction via Anatomically Informed 12-Lead ECG\n",
      "Saved results for: Leveraging Data Science for Optimal Follow-up of Multimorbidity Patients - A Research Protocol\n",
      "Saved results for: Predicting Behavioral Emergencies in the Hospital\n",
      "Saved results for: Development of a Machine Learning Classification Model for ICU Admission Following Resuscitation at a Level Trauma Center\n",
      "Saved results for: Examining Unstructured Free-Text Data from Publicly Accessible Data Streams Gathered from Communities Confronting a Public Health Crisis: A Study on the Predictive Value of Advanced Natural Language Processing-Based Techniques in Real-Time Pandemic Handling\n",
      "Saved results for: A Machine Learning Model Using In-Game Data for Predicting Unhealthy Substance Use Among Adolescents\n",
      "Saved results for: Antihypertensive Drug Repurposing for Dementia Prevention: Target Trial Emulations in Two Large-Scale Electronic Health Record Systems\n",
      "Saved results for: Not So Black and White: Confounders Mediate AI Prediction of Race on Chest X-Rays\n",
      "Finished processing all titles with NaN citation counts.\n",
      "All processing completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from serpapi import GoogleSearch\n",
    "from time import sleep\n",
    "import re\n",
    "\n",
    "def clean_filename(title):\n",
    "    # Remove invalid characters and limit length\n",
    "    clean = re.sub(r'[^\\w\\s-]', '', title).strip()\n",
    "    return clean[:100]  # Limit filename length\n",
    "\n",
    "def search_and_save_results(df, api_key, output_dir='serpapi_results'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Filter for rows where citation_count is NaN\n",
    "    df_nan_citations = df[pd.isna(df['citation_count'])]\n",
    "    \n",
    "    print(f\"Processing {len(df_nan_citations)} titles with NaN citation counts...\")\n",
    "    \n",
    "    for index, row in df_nan_citations.iterrows():\n",
    "        cleaned_title = row['cleaned_title']\n",
    "        filename = clean_filename(cleaned_title)\n",
    "        filename = filename.replace(' ', '_') + '.json'\n",
    "        \n",
    "        params = {\n",
    "            \"api_key\": api_key,\n",
    "            \"engine\": \"google_scholar\",\n",
    "            \"q\": cleaned_title,\n",
    "            \"hl\": \"en\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            \n",
    "            with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            print(f\"Saved results for: {cleaned_title}\")\n",
    "            sleep(2)  # To avoid hitting rate limits\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {cleaned_title}: {str(e)}\")\n",
    "    \n",
    "    print(\"Finished processing all titles with NaN citation counts.\")\n",
    "\n",
    "# Usage\n",
    "api_key = \"\"  # Replace with your actual API key\n",
    "\n",
    "# Load the cleaned dataframes\n",
    "# input_dir = \"processed_data/specifically_cleaned\"\n",
    "# chil_df = pd.read_csv(f\"{input_dir}/chil_semantic_scholar_citations.csv\")\n",
    "# ml4h_df = pd.read_csv(f\"{input_dir}/ml4h_semantic_scholar_citations.csv\")\n",
    "# mlhc_df = pd.read_csv(f\"{input_dir}/mlhc_semantic_scholar_citations.csv\")\n",
    "\n",
    "# Process each dataframe\n",
    "for name, df in [(\"CHIL\", chil_semantic_df), (\"ML4H\", ml4h_semantic_df), (\"MLHC\", mlhc_semantic_df)]:\n",
    "    print(f\"\\nProcessing {name} titles...\")\n",
    "    output_dir = f\"serpapi_results_{name.lower()}_nan_citations\"\n",
    "    search_and_save_results(df, api_key, output_dir)\n",
    "\n",
    "print(\"All processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CHIL titles...\n",
      "Processing 23 titles with NaN citation counts...\n",
      "Saved cleaned CHIL dataframe to processed_data/cleaned_chil_citations.csv\n",
      "Total entries processed: 148\n",
      "Entries with NaN citations: 1\n",
      "Entries without matching SerpAPI results: 1\n",
      "Entries with updated citations: 0\n",
      "\n",
      "Processing ML4H titles...\n",
      "Processing 9 titles with NaN citation counts...\n",
      "Saved cleaned ML4H dataframe to processed_data/cleaned_ml4h_citations.csv\n",
      "Total entries processed: 127\n",
      "Entries with NaN citations: 4\n",
      "Entries without matching SerpAPI results: 4\n",
      "Entries with updated citations: 0\n",
      "\n",
      "Processing MLHC titles...\n",
      "Processing 60 titles with NaN citation counts...\n",
      "Saved cleaned MLHC dataframe to processed_data/cleaned_mlhc_citations.csv\n",
      "Total entries processed: 283\n",
      "Entries with NaN citations: 25\n",
      "Entries without matching SerpAPI results: 25\n",
      "Entries with updated citations: 0\n",
      "All processing completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def clean_filename(title):\n",
    "    # Remove special characters and limit length\n",
    "    clean = re.sub(r'[^\\w\\s-]', '', title).strip()\n",
    "    clean = re.sub(r'\\s+', '_', clean)  # Replace spaces with underscores\n",
    "    return clean[:100]  # Limit filename length\n",
    "\n",
    "def compare_titles(original_title, result_title):\n",
    "    original_tokens = word_tokenize(original_title.lower())\n",
    "    result_tokens = word_tokenize(result_title.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    original_tokens = [token for token in original_tokens if token.isalpha() and token not in stop_words]\n",
    "    result_tokens = [token for token in result_tokens if token.isalpha() and token not in stop_words]\n",
    "    overlapping_words = set(original_tokens) & set(result_tokens)\n",
    "    return len(overlapping_words) >= 3\n",
    "\n",
    "def extract_citations_from_file(file_path, original_title):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    if 'organic_results' in data and len(data['organic_results']) > 0:\n",
    "        result = data['organic_results'][0]\n",
    "        if compare_titles(original_title, result['title']):\n",
    "            return result.get('inline_links', {}).get('cited_by', {}).get('total', 0)\n",
    "    return None\n",
    "\n",
    "def process_dataframe(df, input_dir):\n",
    "    # Filter for rows where citation_count is NaN\n",
    "    df_nan_citations = df[pd.isna(df['citation_count'])]\n",
    "    \n",
    "    print(f\"Processing {len(df_nan_citations)} titles with NaN citation counts...\")\n",
    "    unmatched_count = 0\n",
    "    for index, row in df_nan_citations.iterrows():\n",
    "        cleaned_title = row['cleaned_title']\n",
    "        filename = clean_filename(cleaned_title) + '.json'\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            # Extract citations\n",
    "            citations = extract_citations_from_file(file_path, cleaned_title)\n",
    "            if citations is not None:\n",
    "                df.loc[index, 'citation_count'] = citations\n",
    "            else:\n",
    "                unmatched_count += 1\n",
    "        else:\n",
    "            print(f\"File not found for: {cleaned_title}\")\n",
    "            unmatched_count += 1\n",
    "    \n",
    "    # Drop rows with NaN citation counts\n",
    "    df_cleaned = df.dropna(subset=['citation_count'])\n",
    "    \n",
    "    return df_cleaned, unmatched_count\n",
    "\n",
    "# Usage\n",
    "\n",
    "# Process each dataframe\n",
    "for name, df in [(\"CHIL\", chil_semantic_df), (\"ML4H\", ml4h_semantic_df), (\"MLHC\", mlhc_semantic_df)]:\n",
    "    print(f\"\\nProcessing {name} titles...\")\n",
    "    input_dir = f\"serpapi_results_{name.lower()}_nan_citations\"\n",
    "    df_cleaned, unmatched_count = process_dataframe(df, input_dir)\n",
    "    \n",
    "    # Save the cleaned dataframe\n",
    "    output_file = file_paths[name.lower()]['output']\n",
    "    df_cleaned.to_csv(output_file, index=False)\n",
    "    print(f\"Saved cleaned {name} dataframe to {output_file}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total entries processed: {len(df)}\")\n",
    "    print(f\"Entries with NaN citations: {len(df) - len(df_cleaned)}\")\n",
    "    print(f\"Entries without matching SerpAPI results: {unmatched_count}\")\n",
    "    print(f\"Entries with updated citations: {len(df_cleaned) - (len(df) - len(df[pd.isna(df['citation_count'])]))}\")\n",
    "\n",
    "print(\"All processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
